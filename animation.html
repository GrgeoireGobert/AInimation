<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AInimation</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.8.0/css/bulma.min.css">
	<link href="https://fonts.googleapis.com/css?family=Kaushan+Script&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style_article.css">
    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
  </head>
  <body>
  
  
  <section id="article_hero"  class="hero is-medium is-primary" style="background-image: url(banners/banner_animation_full.jpg);">
  <div class="hero-body">
    <div class="columns">
	<div class="column is-2"></div>
	<div class="column is-8">
      <h1 class="title">
        Animation
      </h1>
      <h2 class="subtitle">
        Générer le mouvement
      </h2>
	</div>
	<div class="column is-2"></div>
    </div>
  </div>
</section>
  
  <div id="burger_menu_full">
	<div id="burger_menu_open" class="burger-menu"><i class="fas fa-bars"></i></div>
	<div id="burger_menu_close" class="burger-menu"><i class="fas fa-times"></i></div>
  </div>
  
  <div id="left_menu" class="left_menu">
	<a id="to_home" href="index.html">AI</a>
	  <nav>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="script.html"><i class="far fa-circle"></i><span class="tooltiptext">Script</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	 
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="storyboard.html"><i class="far fa-circle"></i><span class="tooltiptext">Storyboard</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="modelisation.html"><i class="far fa-circle"></i><span class="tooltiptext">Modélisation</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="texturing.html"><i class="far fa-circle"></i><span class="tooltiptext">Texturing</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="rigging.html"><i class="far fa-circle"></i><span class="tooltiptext">Rigging</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="animation.html"><i class="fas fa-circle"></i><span class="tooltiptext">Animation</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="rendu.html"><i class="far fa-circle"></i><span class="tooltiptext">Rendu</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="son.html"><i class="far fa-circle"></i><span class="tooltiptext">Son</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  </nav>
	 
	<div></div> 
	</div>
	
	<div class="columns">
	  <div class="column is-2"></div>
	  <div class="column is-8">
		<section class="section">
			<div class="container">
			  <h1 class="title">L'animation, qu'est-ce que c'est ?</h1>
				  <h2 class="subtitle">
					<!-- Rien pour l'instant --></br>
				  </h2>
				  
				<p>
				Les outils de rigging permettent de mettre en mouvement un personnage. Cependant, dans une scène, les personnages sont rarement les seuls objets en mouvement : des véhicules peuvent bouger, de l'eau peut couler, des nuages peuvent se former, etc. Certains de ces mouvements peuvent être définis manuellement par des artistes formés à cela. Mais une partie des mouvements ne peut être réalisée à la main car cela demanderait trop de temps pour un résultat final qui manquerait de réalisme. C'est notamment le cas pour l'écoulement d'une rivère, pour le mouvement de personnages secondaires dans une foule, pour le mouvement de grains de poussière, etc. Dans ces cas, il est alors nécessaire de faire appel à des modèles de simulation physique afin de générer automatiquement l'animation. Si cette dernière offre l'avantage d'être hautement réaliste, elle est souvent le fruit de plusieurs heures de calcul.
				</p>
				
				<center>
						<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This work-in-progress test helped with the development of the AI system used by the Crowds Team for Big Hero 6, and later Zootopia, with vehicles and traffic. The system would animate the vehicles complete with passengers to simulate all types of traffic situations. <a href="https://twitter.com/hashtag/TechTuesday?src=hash&amp;ref_src=twsrc%5Etfw">#TechTuesday</a> <a href="https://t.co/Y0IeBBNcnR">pic.twitter.com/Y0IeBBNcnR</a></p>&mdash; Disney Animation (@DisneyAnimation) <a href="https://twitter.com/DisneyAnimation/status/1118204914277072896?ref_src=twsrc%5Etfw">April 16, 2019</a></blockquote>
				</center>
						<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
				
				<p>
				C'est ici que des algorithmes d'intelligence artificielle pourraient être particulièrement utiles. Là où les simulations physiques classiques sont longues à calculer et difficilement paramétrables, des algorithmes d'intelligence artificielle peuvent fournir des résultats approchés bien plus rapidement, ainsi qu'une plus grande diversité de possibilités artistiques. Ce domaine est en pleine expansion, notamment en ce qui concerne la simulation d'écoulements fluides (eau, feu, fumée, etc.). Deux méthodes récentes vont être détaillées par la suite :
				</p>
				<ol>
					<li>Une méthode de résolution des équations descriptives d'un écoulement fluide via un algorithme d'intelligence artifcielle.</li>
					<li>Une méthode de transfert de style à un fluide, là encore grâce à l'intelligence artificielle.</li>
				</ol>
				  
			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container">
			  <h1 class="title">Une simulation de fluide en temps réel</h1>
				  <h2 class="subtitle">
					Résoudre les équations de Navier-Stokes grâce à l'intelligence artificielle
				  </h2>
				  
				
				<p>
				La simulation numérique d'un écoulement fluide se fait classiquement en résolvant les <a href="https://fr.wikipedia.org/wiki/%C3%89quations_de_Navier-Stokes">équations de Navier-Stokes</a> par la méthode des éléments finis (ou d'autres méthodes adaptées). En informatique graphique, ces équations sont un peu simplifiées (écoulement incompressible et fluide non visqueux, la viscosité numérique suffit) car seul un résultat visuellement plausible est désiré. Cependant, même avec ces simplificiations, les simulations de bonne qualité nécessitent plusieurs heures de calcul.<br/>
				Byungsoo Kim et al. <a href="#ref_1">[1]</a> proposent d'obtenir le résultat de la simulation bien plus rapidement grâce à une structure d'intelligence artificielle. De plus, leur méthode respecte, par construction, une partie des équations de Navier-Stokes (la conservation de la masse, équivalente à la divergence nulle du champ des vitesses).<br/><br/>
				</p>
				
				<p>
				Ils ont entraîné un réseau de neurones convolutionnel capable, à partir d'une configuration (position de la source du fluide, vitesse à la source, géométrie de l'environnement, paramètres physiques du fluide), de prédire l'état du fluide à un instant donné de la simulation. Ainsi, si l'écoulement est en 3D, il fournit une grille 3D dont chacune des cases comporte la valeur ponctuelle d'un champ vectoriel.<br/>
				Le réseau de neurones est composé de plusieurs blocs successifs, chacun des blocs étant composé d'une séquence de 5 paires [couche convolutionelle / activation Leaky ReLU].<br/>
				De nombreuses simulations classiques d'écoulements ont déjà été réalisées durant de nombreuses années, ce qui a permis de constituer une base de données d'entraînement. Le réseau est entraîné de manière à fournir un champ vectoriel dont le rotationnel doit être aussi proche que possible du champ de vitesses cible. Cela permet d'avoir un champ de vitesses à divergence nulle (puisque la divergence du rotationel est nulle) et ainsi de vérifier l'équation de conservation de la masse.<br/>
				</p>
				
				<figure class="image" style="width:70%;">
						<img src="images/Fluid_sim_model_CNN_img.png" width="100%"></img>
						<p>La structure du CNN (<a href="https://arxiv.org/pdf/1806.02071.pdf">source</a>). Les chercheurs préconisent N=4-5 et q dépend du rafinement de la simulation.</p>
				</figure>
				
				<p>
				Le réseau de neurones précédent permet d'obtenir la simulation de l'écoulement à un instant donné. Pour pouvoir obtenir la simulation sur un intervalle de temps, un auto-encodeur (encodeur-décodeur) a été développé. L'encodeur a la structure inverse de celle du réseau précédemment décrit tandis que le découdeur correspond au réseau précédent. Ainsi, prenant en entrée un champ de vitesses, l'encodeur le convertit en un vecteur (appartenant à un espace dit "latent") qui est ensuite reconvertit par le décodeur en champ de vitesses (idéalement celui en entrée de l'encodeur).<br/>
				Quel est l'intérêt de ce montage ? Un nouveau réseau de neurones classique (perceptron multi-couches) a ainsi pu être entrainé à générer une séquence de vecteurs latents, permettant d'obtenir le résultat d'une simulation d'écoulement sur un intervalle de temps.
				</p>
				
				<figure class="image" style="width:60%;">
						<img src="images/Fluid_sim_model_auto_encoder_img.png" width="100%"></img>
						<p>La structure de l'auto-encodeur et de la génération d'une simulation sur un intervalle de temps (<a href="https://arxiv.org/pdf/1806.02071.pdf">source</a>).</p>
				</figure>
				
				<p>
				Les résultats obtenus par cette méthode sont excellents, permettant de générer des simulations de très bonne qualité, jusqu'à 700 fois plus rapidement que les méthodes classiques. Les gains en productivité et en ressources sont donc conséquents. Il faut toutefois nuancer ce gain puisque le réseau doit au préalable avoir été entraîné sur la configuration simulée, et ne se généralise donc pas à n'importe quelle configuration d'écoulement.
				</p>

			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container">
			  <h1 class="title">Styliser un écoulement fluide</h1>
				  <h2 class="subtitle">
					Le transfert de style appliqué aux fluides
				  </h2>
				  
				
				<p>
				Le transfert de style est apparu il y a quelques années pour les images (un exemple sur le site <a href="https://deepart.io/#">DeepArt.io</a>). Cela consiste à extraire le style d'une image (comme une oeuvre d'art par exemple) pour l'appliquer sur une autre image (comme un portrait, un paysage, etc.). Ce procédé découle directement des méthodes de classification d'images par réseaux de neurones convolutionnels. En effet, ces réseaux convolutionnels profonds sont performants parce qu'ils sont capables de repérer, dans une image, des structures très précises. Plus une couche se situe profondément dans le réseau, et plus les structures qu'elle est censée détecter sont précises et caractéristiques de l'objet représenté.<br/>
				Les algorithmes de transfert de style s'appuient donc en général sur un réseau de neurones convolutionnel profond dédié à la classification d'images, comme GoogLeNet. L'idée est de faire passer, dans ce réseau, l'image à transformer et l'image contenant le style à appliquer, pour en extraire les <em>feature maps</em> (l'etat des différentes couches du réseau) des couches profondes dans chacun des cas. La sémantique de l'image à transformer est contenue dans ses <em>feature maps</em> tandis que le style de l'autre image est contenu dans le produit scalaire entre ses <em>feature maps</em>.<br/>
				Il suffit ensuite de générer l'image optimale, c'est-à-dire l'image contenant à la fois la sémantique et le style visés, par optimisation (opération classique en machine learning) d'une fonction de perte définie grâce aux <em>feature maps</em>. De plus amples informations sont disponibles <a href="https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398">ici</a>.<br/><br/>
				</p>
				
				<p>
				Récemment, des chercheurs <a href="#ref_2">[2]</a> ont mis au point un algorithme de transfert de style pour la simulation de fumée basé sur le transport, c'est-à-dire basé sur la manipulation du phénomène physique. L'objectif est de contrôler les motifs internes à un écoulement de fumée préexistant en leur appliquant le style souhaité. Pour cela, la méthode ressemble à la méthode utilisée pour le transfert de style sur une image, à quelques modifications près.<br/><br/>
				</p>
				
				<p>
				Les chercheurs ont eu l'idée de chercher, par optimisation, le champ de vitesses qui permettrait de former "naturellement" les motifs souhaités lors de l'écoulement de la fumée. Les étapes sont donc les suivantes :
				</p>
				<ol>
					<li>Tout d'abord, l'image "sémantique" souhaitée est passée dans le réseau préentrainé GoogLeNet, afin d'en extraire les <em>feature maps</em> des couches profondes.</li>
					<li>Ensuite, l'image "stylistique" souhaitée est passée dans le réseau préentrainé GoogLeNet, là encore afin d'en extraire les <em>feature maps</em> des couches profondes.</li>
					<li>Un champ de vitesses différent du champ d'origine est généré. L'écoulement résultant sera donc lui aussi différent.</li>
					<li>Une série de rendus, effectués à différents endroits, sont calculés. Cet ensemble de rendus est lui aussi passé dans le réseau préentrainé GoogLeNet afin d'en extraire un ensemble de  <em>feature maps</em>.</li>
					<li>La fonction de perte est alors évaluée. Elle mesure la distance entre le couple (sémantique + style) des rendus générés et celui des images cibles. Cela est possible par la mesure de la distance euclidienne entre les <em>feature maps</em> pour l'aspect sémantique, et entre les produits scalaires des <em>feature maps</em> pour le style.</li>
					<li>Par rétropropagation du gradient, un nouveau champ de vitesses théoriquement meilleur est calculé, et les étapes 4 à 6 se repètent jusqu'à un optimum.</li>
					<li>Une fois l'optimum atteint, le processus est réitéré avec une discrétisation spatiale plus finale, permettant ainsi d'obtenir un résultat plus précis.</li>
				</ol>
				
				<figure class="image" style="width:90%;">
						<img src="images/Smoke_style_transfer_full_model.png" width="100%"></img>
						<p>La structure globale de la méthode de transfert de style par advection(<a href="http://www.byungsoo.me/project/neural-flow-style/index.html">source</a>)</p>
				</figure>
					
					
				<p>
				Lors du transfert de style sur l'intervalle temporel d'une simulation, la consistance temporelle est assurée directement au niveau de l'évolution temporelle du champ de vitesses. Ce dernier est calculé pour chacun des instants indépendamment, puis un lissage sur une fenêtre temporelle est réalisé.<br/>
				Les possibilités artistiques offertes par cette méthodes sont très vastes, puisqu'il est possible de contrôler de manière précise un écoulement de fumée. La vidéo suivante présente quelques résultats.
				</p>
				
				<figure class="image" style="width:80%;">
						<iframe width="100%" height="400" src="https://www.youtube.com/embed/67qVRhoOQPE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
						<p>Quelques résultats de la méthode de transfert de style par advection(<a href="http://www.byungsoo.me/project/neural-flow-style/index.html">source</a>)</p>
				</figure>
					
				  
			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container refs">
				<h1 class="title">Références</h1>
				<ul>
					<li>[1] : Byungsoo Kim et al. -
						<a id="ref_1" href="https://arxiv.org/pdf/1806.02071.pdf">Deep Fluids: A Generative Network for Parameterized Fluid Simulation</a>
					</li>
					<li>[2] : Byungsoo Kim et al. -
						<a id="ref_2"  href="http://www.byungsoo.me/project/neural-flow-style/index.html">Transport-Based Neural Style Transfer for Smoke Simulations</a>
					</li>
				</ul>
			</div>
		</section>
		
	  </div>
	  <div class="column is-2"></div>
	</div>
	
	<footer class="columns">
		<div class="column is-2"></div>
		<div class="column is-8" id="footer_center">
			<div>© Grégoire GOBERT</div>
			<div>
				<a href="https://twitter.com/gobert_gregoire" style="color:rgb(29,161,242);font-size:1.3em;"><i class="fab fa-twitter"></i></a>
				<a href="https://github.com/GrgeoireGobert" style="color:#333;font-size:1.3em;"><i class="fab fa-github"></i></a>
				<a href="https://www.linkedin.com/in/greg-gobert/" style="color:rgb(40,103,178);font-size:1.3em;"><i class="fab fa-linkedin-in"></i></a>
			</div>
		</div>
		<div class="column is-2"></div>
	</footer>
	
  <script type="text/javascript" src="script.js"></script>
  </body>
</html>