<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AInimation</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.8.0/css/bulma.min.css">
	<link href="https://fonts.googleapis.com/css?family=Kaushan+Script&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style_article.css">
    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
  </head>
  <body>
  
  
  <section id="article_hero"  class="hero is-medium is-primary" style="background-image: url(banners/banner_storyboard_full.jpg);">
  <div class="hero-body">
    <div class="columns">
	<div class="column is-2"></div>
	<div class="column is-8">
      <h1 class="title">
        Storyboard
      </h1>
      <h2 class="subtitle">
        Les premières esquisses
      </h2>
	</div>
	<div class="column is-2"></div>
    </div>
  </div>
</section>
  
  <div id="burger_menu_full">
	<div id="burger_menu_open" class="burger-menu"><i class="fas fa-bars"></i></div>
	<div id="burger_menu_close" class="burger-menu"><i class="fas fa-times"></i></div>
  </div>
  
  <div id="left_menu" class="left_menu">
	<a id="to_home" href="index.html">AI</a>
	 <nav>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="script.html"><i class="far fa-circle"></i><span class="tooltiptext">Script</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	 
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="storyboard.html"><i class="fas fa-circle"></i><span class="tooltiptext">Storyboard</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="modelisation.html"><i class="far fa-circle"></i><span class="tooltiptext">Modélisation</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="texturing.html"><i class="far fa-circle"></i><span class="tooltiptext">Texturing</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="rigging.html"><i class="far fa-circle"></i><span class="tooltiptext">Rigging</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="animation.html"><i class="far fa-circle"></i><span class="tooltiptext">Animation</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="rendu.html"><i class="far fa-circle"></i><span class="tooltiptext">Rendu</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="son.html"><i class="far fa-circle"></i><span class="tooltiptext">Son</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  </nav>
	 
	<div></div>
	</div>
	
	<div class="columns">
	  <div class="column is-2"></div>
	  <div class="column is-8">
		<section class="section">
			<div class="container">
			  <h1 class="title">Le storyboard, qu'est-ce que c'est ?</h1>
				  <h2 class="subtitle">
					<!-- Rien pour l'instant --></br>
				  </h2>
				  
				  <p>
				Une fois le script validé, vient l'étape de création du storyboard. Il s'agit de se faire une première idée de la représentation visuelle du film d'animation. Pour cela, un ensemble de plans simplifiés sont dessinés, à la main, un peu à la manière d'une bande dessinée n'incorporant toutefois pas de bulles de dialogue. Ce storyboard est ensuite converti en une vidéo, les dessins étant séparés d'un intervalle de temps correspond à celui du film réél, et une bande sonore (dialogues, bruitages, musiques) est ajoutée : c'est l'animatique. Cette animatique offre la possibilité d'apprécier le tempo général du film et de le rectifier si besoin.
				</p>
				
				<figure class="image" style="width:80%;">
					<iframe width="100%" height="400" src="https://www.youtube.com/embed/6VvsCB_hVlU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
					<p>Vidéo réalisée par le studio Pixar, comparant l'animatique du film "Toy Story 2" au rendu final.</p>
				</figure>
				
				<p>
				L'intérêt du storyboard et de l'animatique est de pouvoir avoir un premier aperçu de la production finale, afin de rectifier les choix faits lors de l'élaboration du script et de servir de base de travail pour les étapes ultérieures de la production. Il peut donc être intéressant de mettre au point des algorithmes d'intelligence artificielle afin d'assister, voire de remplacer, le processus d'élaboration de storyboard classique. Deux papiers vont être présentés :
				</p>
				<ol>
					<li>Une première technique, qui consiste à transcrire le script en une animation 2D, en venant "découper" et "assembler" des éléments depuis une base de donnée.</li>
					<li>Une seconde technique, qui consiste à transcrire le script en une animation 3D, permettant ainsi d'accéder à des informations plus précises.</li>
				</ol>
				
				  
			</div>
		</section>
		
		<section class="section">
			<div class="container">
			  <h1 class="title">Du script à l'animatique (2D)</h1>
				  <h2 class="subtitle">
					Générer automatiquement l'animatique en lisant le script</br>
				  </h2>
				  
				<p>
				Récemment, des chercheurs (<a href="#ref_1">[1]</a> et <a href="https://arxiv.org/abs/1804.03608">papier original</a>) ont mis au point un algorithme d'intelligence artificielle capable de lire un script, scène par scène, et de générer une vidéo en deux dimensions de chaque scène. Pour cela, ils se sont appuyés sur une base de données de 25 000 séquences vidéo richement annotées extraites de la série d'animation "The Flinstones". Ils ont ainsi pu mettre au point leur méthode, basée sur 3 structures principales :
				</p>
				<ol>
					<li>Une structure (<em>Background Retriever</em>) qui, prenant en entrée la description d'une scène, détecte l'arrière-plan de la scène et en trouve un correspondant dans la base de données.</li>
					<li>Une structure (<em>Layout Composer</em>) qui, prenant en entrée la description de la scène, détecte séquentiellement les différents composants de la scène (personnages, objets) et en déduit leur position et leur échelle dans la séquence vidéo produite.</li>
					<li>Une structure (<em>Entity Retriever</em>) qui, prenant en entrée la description de la scène et la disposition issue de la structure précédente, trouve les animations adéquates de chacun des constituants au sein de la base de données.</li>
				</ol>
				
				<figure class="image" style="width:90%;">
					<img src="images/CRAFT_full_model.png" width="100%"></img>
					<p>La structure générale de la méthode (<a href="https://arxiv.org/abs/1804.03608">source</a>)</p>
				</figure>
				
				<h2 class="subtitle">
					Le <em>Layout Composer</em></br>
				</h2>
				
				<p>
				Comme présenté précédemment, une des étapes nécessaires à la construction de la vidéo asociée à la description de la scène consiste à analyser la description pour en extraire chacun des constituants de la scène, leur position, leur taille et leur positionnement relatif (premier-plan, second-plan, etc.).<br>
				Pour l'analyse de la description de la scène, un réseau de neurones à mémoire est utilisé. Le sens de chacun des mots dépendant de son contexte et de leur agencement (mots précédents et mots suivants), un réseau  bidirectionnel est nécessaire. Le choix d'un Bi-LSTM est donc logique.<br/>
				Cette information sémantique est couplée à l'analyse de la partie de la vidéo déjà construite. La construction étant séquentielle, les différents constituants (personnages, objets) sont ajoutés l'un après l'autre, et il est donc nécessaire de prendre en compte ces informations pour placer le nouveau constituant relativement aux autres. Un réseau de neurones convolutionnel profond a été choisi pour réaliser cette tâche, ce dernier étant particulièrement adapté à l'analyse de structures spatiales.<br/>
				Enfin, les deux informations précédentes sont concaténées et mises en entrée d'un réseau de neurones multi-couches classique afin d'obtenir la position et l'échelle, dans la séquence vidéo construite, du nouveau constituant.
				</p>
				
				<figure class="image" style="width:90%;">
					<img src="images/CRAFT_layout_composer_model.png" width="100%"></img>
					<p>La structure du <em>layout composer</em> (<a href="https://arxiv.org/abs/1804.03608">source</a>)</p>
				</figure>
				
				<h2 class="subtitle">
					L' <em>Entity Retriever</em></br>
				</h2>
				
				<p>
				Une fois que chaque élément est bien positionné par rapport aux autres et est à la bonne échelle, vient l'étape consistant à remplacer chacune des informations abstraites par une animation concrète. Pour cela, il est encore une fois nécessaire d'analyser le sens de la description de la scène, puis de choisir, dans la base de données, la bonne animation pour chacun de ses constituants.<br>
				Pour l'analyse de la description de la scène, un Bi-LSTM est encore logiquement utilisé.<br/>
				La séquence d'informations issue du <em>layout composer</em> (informations de position et de taille des constituants dans chacune des images de la séquence vidéo) sont comparées entre-elles, encore une fois pour en faire une analyse sémantique. Par exemple, si un personnage se déplace lentement (il marche), ou rapidement (il court), l'animation finalement choisie ne sera pas la même. Pour réaliser cette analyse sémantique, chacune des images de la séquence passe dans un réseau de neurones convolutionnel (pour les mêmes raisons que précédemment) et les sorties sont utilisées comme entrée d'un Bi-LSTM, pour tenir compte du "contexte" de la séquence d'images.<br/>
				Parallèlement, une structure analogue réalise les mêmes opérations sur la base de données afin d'obtenir le même type d'informations pour chacune des séquences de la base.<br/>
				Enfin, les séquences obtenues sont comparées (via un produit scalaire) pour déterminer quelle animation de la base de données serait la plus adaptée à la scène en construction.
				</p>
				
				<figure class="image" style="width:90%;">
					<img src="images/CRAFT_entity_retriever_model.png" width="100%"></img>
					<p>La structure de l'<em>entity retriever</em> (<a href="https://arxiv.org/abs/1804.03608">source</a>)</p>
				</figure>
				
				<h2 class="subtitle">
					Le <em>Background Retriever</em></br>
				</h2>
				
				<p>
				Le choix du bon arrière-plan est analogue aux étapes précédentes mais est cependant plus simple. En effet, pour choisir le bon arrière-plan, il est nécessaire de réaliser une analyse sémantique de la description de la scène, pour savoir par exemple si l'arrière plan est en mouvement ou s'il est fixe. Cela est encore une fois réalisé grâce à un Bi-LSTM.<br/>
				Il n'est cette fois plus nécessaire d'obtenir des informations sur la position ou l'échelle de l'arrière-plan. Ce dernier ne passe donc pas par le <em>layout composer</em> et l'étape de sélection dans la base de données (<em>entity retriever</em>) est simplifiée.<br/><br/>
				</p>
				
				<h2 class="subtitle">
					Résultats</br>
				</h2>
				
				<p>
				Une fois que toutes les informations ont été collectées, il n'y a plus qu'à assembler les différents éléments pour obtenir la séquence vidéo issue de la description de la scène. La vidéo suivante présente les résultats obtenus :
				</p>
				
				<figure class="image" style="width:80%;">
					<iframe width="100%" height="400" src="https://www.youtube.com/embed/688Vv86n0z8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
					<p>Résultats obtenus par la méthode présentée. Même si cela ne fonctionne pas toujours, les résutats sont plutôt encourageants.</p>
				</figure>
				
				<p>
				Le but d'un storyboard étant d'avoir un premier aperçu de la structure du film d'animation, ce type d'algorithme d'intelligence artificielle peut être utile pour générer rapidement un premier aperçu à partir du script. Cependant, il ne permet pas de prendre en compte les choix artistiques (comme le point de vue dans une scène, ou d'autres informations plus complexes) qui peuvent pourtant être normalement analysés et corrigés grâce au storyboard.
				</p>
				
			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container">
			  <h1 class="title">Du script à l'animation (3D)</h1>
				  <h2 class="subtitle">
					Générer automatiquement l'animation en lisant le script</br>
				  </h2>
				  
				<p>
				Des chercheurs des laboratoires de Disney <a href="#ref_2">[2]</a><a href="#ref_3">[3]</a> ont mis au point une méthode, liant intelligence artificielle et algorithmie classique, afin de générer une animation 3D à partir du script. L'avantage de ce travail par rapport au précédent réside dans le fait qu'il est ici possible de placer la caméra n'importe où dans la scène.<br/>
				La méthode se décompose en 3 étapes principales :
				</p>
				<ol>
					<li>Tout d'abord, une étape d'analyse du script permet de catégoriser les indications (lieu, action, bruitage, ect.).</li>
					<li>Ensuite vient une étape d'analyse sémantique et de décomposition des descriptions complexes en un ensemble de descriptions simples.</li>
					<li>Enfin, ces descriptions simples sont analysées et transcriptes en animation 3D.</li>
				</ol>
				
				<figure class="image" style="width:90%;">
					<img src="images/Disney_Generation_model_full.png" width="100%"></img>
					<p>La structure globale de la méthode (<a href="https://studios.disneyresearch.com/2019/06/01/generating-animations-from-screenplays/">source</a>)</p>
				</figure>
				
				<p>
				La première étape d'extraction des informations du script utiles pour la génération de l'animation peut se faire de deux manières. <br/>
				Si le script est "bien formé", c'est-à-dire s'il respecte une structure prédéfinie, alors une machine à états finis est utilisée en parcourant le document afin d'y déceler les informations utiles.<br/>
				Un modèle SVM a été mis au point pour le cas où le script est "mal formé", avec de bons résultats (92% de succès).<br/>
				Un algorithme d'intelligence artificielle est ensuite utilisé afin de résoudre les problèmes de coréférence. Le but est de remplacer les pronoms par les noms qu'ils désignent. Cela permettra ensuite de distinguer plus facilement les différentes actions présentes au sein d'une même description.
				</p>
				
				<figure class="image" style="width:60%;">
					<img src="images/Disney_Generation_FSM.png" width="100%"></img>
					<p>La machine à états finis utilisée pour l'extraction d'informations utiles, dans le cas d'un script bien formé (<a href="https://studios.disneyresearch.com/2019/06/01/generating-animations-from-screenplays/">source</a>)</p>
				</figure>
				
				<p>
				La simplificiation et la décomposition des descriptions complexes en un ensemble de descriptions plus simples se fait ensuite en utilisant des règles de structures syntaxiques (constructions passives, phrases apposées, propositions relatives, propositions coordonnées, propositions corrélées et propositions adverbiales). Pour chacune des phrases, la détection et le traitement des 6 cas précédents sont réalisés en utilisant notamment des réseaux de neurones convolutionnels.<br>
				Suite à cela, le vocabulaire des descriptions simples générées est lui aussi simplifié, en utilisant un dictionnaire de synonymes, afin de prendre en compte la diversité limitée d'objets et d'actions réalisables dans l'animation 3D finalement générée.<br/>
				Enfin, une analyse sémantique des descriptions textuelles simples est réalisée afin de générer l'action à jouer dans l'animation finale. Pour cela, les chercheurs ont eux-aussi mis au point un Bi-LSTM en faisant du <em>transfer learning</em> à partir du réseau ELMo pré-entrainé.<br/><br/>
				</p>
				
				<p>
				Puis, la dernière étape, consistant à générer l'animation 3D, est réalisée en utilisant un moteur de jeu associé à une bibliothèque d'objets 3D et d'animations pré-enregistrées. Malheureusement, aucune vidéo des résultats obtenus est jointe à la publication. Selon une campagne menée par les chercheurs, environ 70% des personnes interrogées sur la qualité de l'animation obtenue ont un avis positif. Cette méthode a sûrement besoin d'être amliorée, mais elle présente un potentiel important dans le processus de création du scénario d'un film d'animation.
				</p>
				
			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container refs">
				<h1 class="title">Références</h1>
				<ul>
					<li>[1] : Analytics Vidhya -
						<a id="ref_1" href="https://www.analyticsvidhya.com/blog/2018/04/this-ai-create-cartoons-text-description/">Thanks to AI, you can now create Cartoons from Text Based Descriptions</a>
					</li>
					<li>[2] : Venture Beat -
						<a id="ref_2"  href="https://venturebeat.com/2019/04/12/disneys-ai-generates-storyboard-animations-from-screenplays/">Disney’s AI generates storyboard animations from scripts</a>
					</li>
					<li>[3] : Disney Research -
						<a id="ref_3"  href="https://studios.disneyresearch.com/2019/06/01/generating-animations-from-screenplays/">Generating Animations from Screenplays</a>
					</li>
				</ul>
			</div>
		</section>
		
	  </div>
	  <div class="column is-2"></div>
	</div>
	
	<footer class="columns">
		<div class="column is-2"></div>
		<div class="column is-8" id="footer_center">
			<div>© Grégoire GOBERT</div>
			<div>
				<a href="https://twitter.com/gobert_gregoire" style="color:rgb(29,161,242);font-size:1.3em;"><i class="fab fa-twitter"></i></a>
				<a href="https://github.com/GrgeoireGobert" style="color:#333;font-size:1.3em;"><i class="fab fa-github"></i></a>
				<a href="https://www.linkedin.com/in/greg-gobert/" style="color:rgb(40,103,178);font-size:1.3em;"><i class="fab fa-linkedin-in"></i></a>
			</div>
		</div>
		<div class="column is-2"></div>
	</footer>
	
  <script type="text/javascript" src="script.js"></script>
  </body>
</html>