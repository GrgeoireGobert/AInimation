<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AInimation</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.8.0/css/bulma.min.css">
	<link href="https://fonts.googleapis.com/css?family=Kaushan+Script&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style_article.css">
    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
  </head>
  <body>
  
  
  <section id="article_hero"  class="hero is-medium is-primary" style="background-image: url(banners/banner_rendu_full.png);">
  <div class="hero-body">
    <div class="columns">
	<div class="column is-2"></div>
	<div class="column is-8">
      <h1 class="title">
        Rendu
      </h1>
      <h2 class="subtitle">
        A la recherche de la lumi√®re
      </h2>
	</div>
	<div class="column is-2"></div>
    </div>
  </div>
</section>
  
  <div id="burger_menu_full">
	<div id="burger_menu_open" class="burger-menu"><i class="fas fa-bars"></i></div>
	<div id="burger_menu_close" class="burger-menu"><i class="fas fa-times"></i></div>
  </div>
  
  <div id="left_menu" class="left_menu">
	<a id="to_home" href="index.html">AI</a>
	  <nav>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="script.html"><i class="far fa-circle"></i><span class="tooltiptext">Script</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	 
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="storyboard.html"><i class="far fa-circle"></i><span class="tooltiptext">Storyboard</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="modelisation.html"><i class="far fa-circle"></i><span class="tooltiptext">Mod√©lisation</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="texturing.html"><i class="far fa-circle"></i><span class="tooltiptext">Texturing</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="rigging.html"><i class="far fa-circle"></i><span class="tooltiptext">Rigging</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="animation.html"><i class="far fa-circle"></i><span class="tooltiptext">Animation</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="rendu.html"><i class="fas fa-circle"></i><span class="tooltiptext">Rendu</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="son.html"><i class="far fa-circle"></i><span class="tooltiptext">Son</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  </nav>
	 
	<div></div>
	</div>
	
	<div class="columns">
	  <div class="column is-2"></div>
	  <div class="column is-8">
		<section class="section">
			<div class="container">
			  <h1 class="title">Le rendu, qu'est-ce que c'est ?</h1>
				  <h2 class="subtitle">
					<!-- Rien pour l'instant --></br>
				  </h2>
				  
				<p>
				Les √©tapes pr√©c√©dentes ont permis de mod√©liser num√©riquement les diff√©rentes sc√®nes : position et cat√©gorie des sources de lumi√®re, position, orientation et d√©formation des objets, simulation des ph√©nom√®nes physiques, etc. Il est alors temps de synth√©tiser la s√©quence d'images, prises √† un intervalle de temps r√©gulier (au moins 24 images par seconde), pour aboutir √† une s√©quence vid√©o. Cette synth√®se d'image se fait gr√¢ce au <em>Ray Tracing</em> (plus pr√©cis√©ment au <a href="https://fr.wikipedia.org/wiki/Path_tracing"><em>Path Tracing</em></a>) qui consiste √† lancer dans la sc√®ne 3D des rayons partant de la cam√©ra. Ces rayons rebondissent sur les diff√©rents objets et acqui√®rent (lorsqu'ils rencontrent une source d'√©nergie) ou perdent (lorsqu'ils rencontrent un mat√©riau absorbant) de l'√©nergie sur leur trajet. La direction de rebond peut d√©pendre du mat√©riau (par exemple pour un miroir parfait) ou peut √™tre al√©atoire (si le but est d'estimer l'illumination globale en un point). Les informations ainsi r√©colt√©es permettent de synth√©tiser une image r√©aliste, au prix toutefois de nombreuses heures de calcul.
				</p>
				
				<figure class="image" style="width:50%;">
						<img src="images/Rendu_RayTracing_ECL.png" width="100%"></img>
						<p>Exemple de rendu r√©aliste simple r√©alis√© √† l'Ecole Centrale de Lyon, par la m√©thode de <em>Path Tracing</em>. La sph√®re centrale blanche est color√©e par les murs environnants gr√¢ce au calcul d'illumination globale.</p>
				</figure>
				
				<p>
				Comme cela est visible sur l'image pr√©c√©dente, le caract√®re al√©atoire de certains rayons (d√ª √† l'estimation de l'illumination globale par la <a href="https://fr.wikipedia.org/wiki/M%C3%A9thode_de_Monte-Carlo">m√©thode de Monte Carlo</a>) engendre une image bruit√©e. Une des parades √† ce probl√®me est la r√©alisation d'une estimation plus pr√©cise en lan√ßant plus de rayons. Cependant, pour r√©duire la quantit√© de bruit par deux, il est n√©cessaire de multiplier par quatre le nombre de rayons lanc√©s et donc le temps de calcul. Une autre solution consiste √† utiliser l'intelligence artificielle afin de d√©bruiter efficacement l'image obtenue. Cette solution sera pr√©sent√©e par la suite.<br/><br/>
				</p>
				
				<p>
				Pour certaines productions, les choix artistiques ajoutent des contraintes √† l'√©tape de rendu. C'est par exemple le cas du r√©cent film d'animation <em>Spider-Man: New Generation</em> du studio Sony Pictures Animation, qui pr√©sentait un aspect "comics" dans ses rendus. L√† encore, l'intelligence artificielle est venue en aide aux artistes afin d'arriver √† leurs fins.<br/>
				Un vaste champ de possibilit√©s artistiques s'ouvre avec l'arriv√©e r√©cente du transfert de style, permettant ainsi de se lib√©rer de la contrainte de mat√©riaux physiques (voir la page consacr√©e au <a href="texturing.html">texturing</a>) au b√©n√©fice de styles plus exotiques.<br/>
				Ces deux avanc√©es, obtenues gr√¢ce √† l'intelligence artificielle, seront pr√©sent√©es dans les paragraphes suivants.
				</p>
				
				
				  
			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container">
			  <h1 class="title">Le d√©bruitage des rendus</h1>
				  <h2 class="subtitle">
					Corriger les erreurs des estimations de Monte Carlo</br>
				  </h2>
				  
				<p>
				Comme cela a √©t√© expliqu√© en introduction, la synth√®se de rendu par <em>Path Tracing</em> g√©n√®re des r√©sultats physiquement tr√®s justes mais bruit√©s, m√™me en lan√ßant un tr√®s grand nombre de rayons pour le calcul de chaque pixel, √† cause de l'estimation de l'illumination globale par la m√©thode de Monte Carlo. Ce bruit √©tant in√©vitable, une √©tape de filtrage est n√©cessaire afin de r√©duire son impact. La solution classique, visant √† remplacer la valeur d'un pixel de l'image par une moyenne pond√©r√©e des valeurs de ses voisins, pr√©sente un inconv√©nient majeur : comment choisir les pond√©rations adapt√©es alors que la structure du bruit varie d'un endroit √† l'autre de l'image (√† cause de la non-uniformit√© de l'illumination globale) ?<br/>
				Des chercheurs du studio Disney [<a href="#ref_1">1</a>,<a href="#ref_1">2</a>] ont tent√© de r√©soudre ce probl√®me en utilisant des algorithmes d'intelligence artificielle. L'objectif √©tait de mettre au point un r√©seau de neurones convolutionnel capable de pr√©dire le filtre (aussi appel√© "noyau") adapt√© √† chacun des pixels de l'image. Ainsi, l'√©tape de filtrage peut ensuite devenir "dynamique" dans le sens o√π la structure locale du bruit est prise en compte.<br/><br/>		
				</p>
				
				<p>
				L'architecture de la solution consiste en r√©alit√© en deux r√©seaux de neurones convolutionnels : un r√©seau d√©di√© √† la composante diffuse de l'image (part isotrope de la r√©flexion de la lumi√®re) et un autre r√©seau d√©di√© √† la composante sp√©culaire (part anisotrope de la r√©flexion de la lumi√®re). Les √©tapes de la m√©thode sont les suivantes :
				</p>
				<ol>
					<li>S√©paration des donn√©es g√©n√©r√©es par le moteur de rendu : d'un c√¥t√© les informations diffuses, de l'autre les informations sp√©culaires.</li>
					<li>Pr√©-traitement des donn√©es adapt√© √† la cat√©gorie (diffus ou sp√©culaire) : normalisation, passage au logarithme, etc.</li>
					<li>Passage de chacun des pixels (et de son voisinnage) dans les r√©seaux de neurones et application du filtrage.</li>
					<li>Post-traitement des deux images obtenues, visant √† annuler le pr√©-traitement r√©alis√©.</li>
					<li>Fusion des deux composantes (diffuse et sp√©culaire) et obtention de l'image finale d√©bruit√©e.</li>
				</ol>
				
				
				<figure class="image" style="width:90%;">
						<img src="images/Denoising_model_full.png" width="100%"></img>
						<p>Architecture globale de la m√©thode de d√©bruitage par g√©n√©ration de noyaux (<a href="https://studios.disneyresearch.com/2017/07/20/kernel-predicting-convolutional-networks-for-denoising-monte-carlo-renderings/">source</a>)</p>
				</figure>
				
				<p>
				Chacun des deux r√©seaux de neurones convolutionels est compos√©e de 8 couches convolutionnelles cach√©es avec 100 filtres 5x5 √† chaque couche. La fonction d'activation ReLu est utilis√©e en sortie de chaque couche. Le r√©seau prend en entr√©e les informations sur un pixel et son voisinage, et il fournit en sortie un noyau de taille 21x21, les pond√©rations √©tant finalement obtenues par normalisation (Softmax). Il suffit ensuite d'appliquer ce filtre au pixel. La fonction de perte choisie est la norme 1 entre le pixel g√©n√©r√© et le pixel cible. L'optimisation se fait de mani√®re classique.
				</p>
				
				<p>
				La jeu d'entrainement est constitu√© de 600 images calcul√©es par <em>Path Tracing</em> avec plusieurs milliers de rayons par pixel (donc des images tr√®s peu bruit√©es) ayant demand√© de nombreuses heures de calcul. Chacune des images est ensuite d√©coup√©e en patchs. Les r√©sultats sont tr√®s bons puisqu'√† partir d'une image synth√©tis√©e avec 32 rayons par pixel, puis d√©bruit√©e en 12 secondes, il est possible d'obtenir un r√©sultat visuellement √©quivalent voire meilleur que la m√™me image synth√©tis√©e avec 128 rayons par pixel ! Les gains de temps sont donc consid√©rables, ce qui explique l'emploi massif de telles m√©thodes de d√©bruitage dans l'industrie du cin√©ma d'animation aujourd'hui.
				</p>
				
				<figure class="image" style="width:100%;">
						<img src="images/Denoising_test_train.png" width="100%"></img>
						<p>(A gauche) Le jeu d'entra√Ænement, compos√© d'images g√©n√©r√©es √† 32 rayons par pixel, et de leur √©quivalent √† 1024 rayons par pixel. (A droite) L'application de l'algorithme de d√©bruitage sur une image √† 32 rayons par pixel : le r√©sultat est visuellement √©quivalent √† une image √† 1024 rayons par pixel (<a href="https://studios.disneyresearch.com/2017/07/20/kernel-predicting-convolutional-networks-for-denoising-monte-carlo-renderings/">source</a>)</p>
				</figure>
				
				  
			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container">
			  <h1 class="title">Trac√© intelligent de contours</h1>
				  <h2 class="subtitle">
					Le machine learning pour aller aux Oscars</br>
				  </h2>
				  
				  <center>
				  <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Congratulations to the <a href="https://twitter.com/SpiderVerse?ref_src=twsrc%5Etfw">@SpiderVerse</a> cast and crew on their <a href="https://twitter.com/hashtag/Oscars?src=hash&amp;ref_src=twsrc%5Etfw">#Oscars</a> nomination for Best Animated Feature! üï∑Ô∏èüéâ <a href="https://t.co/pstwTwNdHT">pic.twitter.com/pstwTwNdHT</a></p>&mdash; Sony Pictures Animation (@SonyAnimation) <a href="https://twitter.com/SonyAnimation/status/1087712954819108865?ref_src=twsrc%5Etfw">January 22, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
				  </center>
				  
				  
				<p>
				Lors de la production du film d'animation <em>Spiderman: Into the Spiderverse </em>, le studio Sony Pictures Imageworks a souhait√© appliquer un style particulier √† ses rendus, proche des illustrations des "comics". Cela n√©cessitait, entre autres, de superposer au rendu des "lignes d'encre" au niveau de certains contours (joues, nez, yeux, etc.). Cet effet est visible sur l'illustration du tweet ci-dessus. La difficult√© r√©side dans le fait que ces lignes d√©pendent de l'angle de vue et de l'expression faciale du personnage.<br/>
				Dans un premier temps, un outil manuel a √©t√© d√©velopp√©, permettant aux artistes de dessiner ces "lignes d'encre" sur le rendu. Cependant, ce processus devant √™tre fait image par image, cela demandait beaucoup de temps. C'est pourquoi des chercheurs du studio <a href="#ref_3">[3]</a> ont d√©velopp√© un outil d'intelligence artificielle, permettant de calculer automatiquement ces "lignes d'encre". Les artistes n'avaient alors plus qu'√† r√©aliser les ajustements n√©cessaires.<br/><br/>
				</p>
				
				<p>
				Les maillages 3D, utilis√©s dans le secteur du cin√©ma d'animation, pr√©sentent l'avantage de pouvoir √™tre "mis √† plat" (passage du monde 3D √† un espace 2D d√©formant le moins possible le maillage) par le biais d'une <em>param√©trisation</em>.<br/>
				Les d√©tails sur la structure de l'algorithme utilis√© n'ont pas √©t√© d√©voil√©s par le studio. Ce dernier prend en entr√©e des informations sur l'orientation relative de la t√™te du personnage par rapport √† la cam√©ra ainsi que des informations sur l'expression faciale du personnage. Il fournit en sortie un ensemble de "lignes d'encre" dans l'espace de param√©trisation 2D, qu'il n'y a alors plus qu'√† projeter dans le monde 3D par l'op√©ration inverse.<br/><br/>
				</p>
				
				<p>
				Le jeu d'entrainement a √©t√© r√©alis√© en plusieurs √©tapes :
				</p>
				<ol>
					<li>Tout d'abord, des artistes ont d√©ssin√© les "lignes d'encre" correspondant √† 600 poses (position et expression faciale) diff√©rentes de diff√©rents personnages.</li>
					<li>Un premier algorithme a √©t√© entra√Æn√© sur ce jeu de donn√©es.</li>
					<li>Cet algorithme a √©t√© utilis√© pour g√©n√©rer automatiquement une nouvelle s√©rie de "lignes d'encre" qui ont √©t√© corrig√©es par des artistes si n√©cessaire.</li>
					<li>Cet ensemble de "lignes d'encre" agrandi a permis d'entra√Æner un second algorithme plus performant que le premier. C'est ce dernier qui a finalement √©t√© utilis√© lors de la production.</li>
				</ol>
				
				
				<figure class="image" style="width:70%;">
						<img src="images/miles_inklines_crop.jpg" width="100%"></img>
						<p>Un exemple de r√©sultat final obtenu (<a href="https://www.fxguide.com/fxfeatured/ink-lines-and-machine-learning/?utm_source=twitter&utm_medium=social&utm_campaign=SocialWarfare">source</a>)</p>
				</figure>
				
				<p>
				L'intelligence artificielle a encore une fois √©t√© mise au service de l'artistique, permettant d'automatiser des t√¢ches r√©p√©titives de haut niveau. Aucune solution classique n'avait permis d'obtenir des r√©sultats √©quivalents.
				</p>
				  
			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container">
			  <h1 class="title">Le transfert de style</h1>
				  <h2 class="subtitle">
					Une multitude de possibilit√©s</br>
				  </h2>
				  
				<p>
				Le <em>Path Tracing</em> n√©cessite de d√©crire physiquement le comportement de la mati√®re et permet d'obtenir des rendus tr√®s r√©alistes. Il est cependant tr√®s compliqu√© d'adapter cette technique dans le but d'obtenir un style artistique particulier. C'est dans ce cas que les r√©centes d√©couvertes en terme de transfert de style peuvent √™tre int√©ressantes.<br/><br/>
				</p>
				
				<p>
				En 2015, des chercheurs de l'universit√© de T√ºbingen <a href="#ref_4">[4]</a> ont exploit√© les avanc√©es dans le domaine de la classification d'images par r√©seaux de neurones profonds afin de cr√©er un algorithme de transfert de style tr√®s efficace. Ces r√©seaux de neurones profonds, d√©di√©s √† la classification d'images, sont constitu√©s d'un empilement de couches convolutionelles. A son arriv√©e dans chacune des couches, la sortie (l'√©quivalent d'une image) de la couche pr√©c√©dente subit plusieurs op√©rations de filtrage, menant √† un ensemble de <em>feature maps</em> (l'√©quivalent d'une image) en sortie. Ces <em>feature maps</em> ont la particularit√© de comporter des informations pr√©cises sur le contenu et le style d'une image. Le contenu est d√©crit par la valeur des <em>feature maps</em>, les <em>feature maps</em> des couches profondes permettent d'identifier des concepts propres au contenu, particuli√®rement discriminants. Le style est d√©crit par la corr√©lation entre les <em>feature maps</em> d'une m√™me couche.<br/>
				Ainsi, pour transf√©rer le style d'une image sur une autre image, tout en conservant son contenu, il s'agit de r√©soudre un probl√®me d'optimisation sur une fonction de perte d√©finie √† partie de ces <em>feature maps</em>. Un exemple de r√©sultat est disponible sur l'image suivante.
				</p>
				
				<figure class="image" style="width:80%;">
						<img src="images/DeepArt_example.png" width="100%"></img>
						<p>Un exemple d'application de la m√©thode de transfert de style (<a href="https://arxiv.org/pdf/1508.06576.pdf">source</a>). Plusieurs styles artistiques (B,C,D) sont appliqu√©s √† l'image d'origine (A).</p>
				</figure>
				
				<p>
				Les perspectives offertes par cette m√©thode dans le secteur du cin√©ma d'animation paraissent int√©ressantes. Cependant, d'autres techniques, qui ne sont pas bas√©es sur des algorithmes d'intelligence artificielle, permettent d'obtenir de meilleurs r√©sultats. C'est le cas de StyleLit <a href="#ref_5">[5]</a>, mis au point par des chercheurs de l'universit√© de Prague. Leur algorithme exploite les informations de "chemin d'un rayon lumineux" fournies par le moteur de rendu. Par exemple, un rayon lumineux arrivant sur l'oeil peut √™tre parti d'une source de lumi√®re (L), avoir subi 2 rebonds diffus (D) puis un rebond sp√©culaire (S) avant d'atteindre l'oeil (E). Son chemin sera alors d√©crit par l'expression <em>LDDSE</em>.<br/>
				Il suffit alors √† un artiste de dessiner une sc√®ne simple avec le style voulu, puis de g√©n√©rer les expressions des chemins pour cette sc√®ne simple, pour avoir une table de correspondance entre expression et style. Il suffit ensuite d'appliquer ces styles aux endroits appropri√©s pour n'importe quelle nouvelle sc√®ne.
				</p>
				
				<figure class="image" style="width:90%;">
						<img src="images/StyleLit_example.png" width="100%"></img>
						<p>R√©sultats obtenus sans usage d'intelligence artificielle (<a href="https://dcgi.fel.cvut.cz/home/sykorad/stylit">source</a>). (Premi√®re image) La sc√®ne de r√©f√©rence. (Images suivantes) L'artiste dessine la sc√®ne simple (sph√®re dans l'encart en haut √† droite de chaque image) et le style est appliqu√© √† la sc√®ne. </p>
				</figure>
				
				<p>
				Les algorithmes de transfert de style utilisant les r√©seaux de neurones profonds d√©di√©s √† la classification n'ont pas acc√©s √† l'information de "chemin d'un rayon lumineux". Il pourrait toutefois √™tre envisageable d'am√©liorer leurs r√©sultats en exploitant ces informations, sp√©cifiquement pour le cin√©ma d'animation.
				</p>
				  
			</div>
		</section>	
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container refs">
				<h1 class="title">R√©f√©rences</h1>
				<ul>
					<li>[1] : DisneyResearch -
						<a id="ref_1" href="https://studios.disneyresearch.com/2017/07/20/kernel-predicting-convolutional-networks-for-denoising-monte-carlo-renderings/">Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings</a>
					</li>
					<li>[2] : DisneyResearch -
						<a id="ref_2"  href="https://studios.disneyresearch.com/2018/07/30/denoising-with-kernel-prediction-and-asymmetric-loss-functions/">Denoising with Kernel Prediction and Asymmetric Loss Functions</a>
					</li>
					<li>[3] : FXGuide -
						<a id="ref_3"  href="https://www.fxguide.com/fxfeatured/ink-lines-and-machine-learning/?utm_source=twitter&utm_medium=social&utm_campaign=SocialWarfare">Ink Lines and Machine Learning</a>
					</li>
					<li>[4] : Leon A. Gatys et al. -
						<a id="ref_4"  href="https://arxiv.org/pdf/1508.06576.pdf">A Neural Algorithm of Artistic Style</a>
					</li>
					<li>[5] : Jakub Fi≈°er et al. -
						<a id="ref_5"  href="https://dcgi.fel.cvut.cz/home/sykorad/stylit">StyLit: Illumination-Guided Example-Based Stylization of 3D Renderings</a>
					</li>
				</ul>
			</div>
		</section>
		
	  </div>
	  <div class="column is-2"></div>
	</div>
	
	<footer class="columns">
		<div class="column is-2"></div>
		<div class="column is-8" id="footer_center">
			<div>¬© Gr√©goire GOBERT</div>
			<div>
				<a href="https://twitter.com/gobert_gregoire" style="color:rgb(29,161,242);font-size:1.3em;"><i class="fab fa-twitter"></i></a>
				<a href="https://github.com/GrgeoireGobert" style="color:#333;font-size:1.3em;"><i class="fab fa-github"></i></a>
				<a href="https://www.linkedin.com/in/greg-gobert/" style="color:rgb(40,103,178);font-size:1.3em;"><i class="fab fa-linkedin-in"></i></a>
			</div>
		</div>
		<div class="column is-2"></div>
	</footer>
	
  <script type="text/javascript" src="script.js"></script>
  </body>
</html>