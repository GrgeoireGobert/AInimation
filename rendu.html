<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AInimation</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.8.0/css/bulma.min.css">
	<link href="https://fonts.googleapis.com/css?family=Kaushan+Script&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style_article.css">
    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
  </head>
  <body>
  
  
  <section id="article_hero"  class="hero is-medium is-primary" style="background-image: url(banners/banner_rendu_full.png);">
  <div class="hero-body">
    <div class="columns">
	<div class="column is-2"></div>
	<div class="column is-8">
      <h1 class="title">
        Rendu
      </h1>
      <h2 class="subtitle">
        A la recherche de la lumière
      </h2>
	</div>
	<div class="column is-2"></div>
    </div>
  </div>
</section>
  
  <div id="burger_menu_full">
	<div id="burger_menu_open" class="burger-menu"><i class="fas fa-bars"></i></div>
	<div id="burger_menu_close" class="burger-menu"><i class="fas fa-times"></i></div>
  </div>
  
  <div id="left_menu" class="left_menu">
	<a id="to_home" href="index.html">AI</a>
	  <nav>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="script.html"><i class="far fa-circle"></i><span class="tooltiptext">Script</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	 
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="storyboard.html"><i class="far fa-circle"></i><span class="tooltiptext">Storyboard</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="modelisation.html"><i class="far fa-circle"></i><span class="tooltiptext">Modélisation</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="texturing.html"><i class="far fa-circle"></i><span class="tooltiptext">Texturing</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="rigging.html"><i class="far fa-circle"></i><span class="tooltiptext">Rigging</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="animation.html"><i class="far fa-circle"></i><span class="tooltiptext">Animation</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="rendu.html"><i class="fas fa-circle"></i><span class="tooltiptext">Rendu</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  <div class="point"><a href="son.html"><i class="far fa-circle"></i><span class="tooltiptext">Son</span></a></div>
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  
	  <i class="fas fa-ellipsis-v" style="font-size:0.5em;"></i>
	  </nav>
	 
	<div></div>
	</div>
	
	<div class="columns">
	  <div class="column is-2"></div>
	  <div class="column is-8">
		<section class="section">
			<div class="container">
			  <h1 class="title">Le rendu, qu'est-ce que c'est ?</h1>
				  <h2 class="subtitle">
					<!-- Rien pour l'instant --></br>
				  </h2>
				  
				<p>
				Les étapes précédentes ont permis de modéliser numériquement les différentes scènes : position et catégorie des sources de lumière, position, orientation et déformation des objets, simulation des phénomènes physiques, etc. Il est alors temps de synthétiser la séquence d'images, prises à un intervalle de temps régulier (au moins 24 images par seconde), pour aboutir à une séquence vidéo. Cette synthèse d'image se fait grâce au <em>Ray Tracing</em> (plus précisément au <a href="https://fr.wikipedia.org/wiki/Path_tracing"><em>Path Tracing</em></a>) qui consiste à lancer dans la scène 3D des rayons partant de la caméra. Ces rayons rebondissent sur les différents objets et acquièrent (lorsqu'ils rencontrent une source d'énergie) ou perdent (lorsqu'ils rencontrent un matériau absorbant) de l'énergie sur leur trajet. La direction de rebond peut dépendre du matériau (par exemple pour un miroir parfait) ou peut être aléatoire (si le but est d'estimer l'illumination globale en un point). Les informations ainsi récoltées permettent de synthétiser une image réaliste, au prix toutefois de nombreuses heures de calcul.
				</p>
				
				<figure class="image" style="width:50%;">
						<img src="images/Rendu_RayTracing_ECL.png" width="100%"></img>
						<p>Exemple de rendu réaliste simple réalisé à l'Ecole Centrale de Lyon, par la méthode de <em>Path Tracing</em>. La sphère centrale blanche est colorée par les murs environnants grâce au calcul d'illumination globale.</p>
				</figure>
				
				<p>
				Comme cela est visible sur l'image précédente, le caractère aléatoire de certains rayons (dû à l'estimation de l'illumination globale par la <a href="https://fr.wikipedia.org/wiki/M%C3%A9thode_de_Monte-Carlo">méthode de Monte Carlo</a>) engendre une image bruitée. Une des parades à ce problème est la réalisation d'une estimation plus précise en lançant plus de rayons. Cependant, pour réduire la quantité de bruit par deux, il est nécessaire de multiplier par quatre le nombre de rayons lancés et donc le temps de calcul. Une autre solution consiste à utiliser l'intelligence artificielle afin de débruiter efficacement l'image obtenue. Cette solution sera présentée par la suite.<br/><br/>
				</p>
				
				<p>
				Pour certaines productions, les choix artistiques ajoutent des contraintes à l'étape de rendu. C'est par exemple le cas du récent film d'animation <em>Spider-Man: New Generation</em> du studio Sony Pictures Animation, qui présentait un aspect "comics" dans ses rendus. Là encore, l'intelligence artificielle est venue en aide aux artistes afin d'arriver à leurs fins.<br/>
				Un vaste champ de possibilités artsitiques s'ouvre avec l'arrivée récente du transfert de style, permettant ainsi de se libérer de la contrainte de matériaux physiques (voir la page consacrée au <a href="texturing.html">texturing</a>) au bénéfice de styles plus exotiques.<br/>
				Ces deux avancées, obtenues grâce à l'intelligence artificielle, seront présentées dans les paragraphes suivants.
				</p>
				
				
				  
			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container">
			  <h1 class="title">Le débruitage des rendus</h1>
				  <h2 class="subtitle">
					Corriger les erreurs des estimations de Monte Carlo</br>
				  </h2>
				  
				<p>
				Comme cela a été expliqué en introduction, la synthèse de rendu par <em>Path Tracing</em> génère des résultats physiquement très justes mais bruités, même en lançant un très grand nombre de rayons pour le calcul de chaque pixel, à cause de l'estimation de l'illumination globale par la méthode de Monte Carlo. Ce bruit étant inévitable, une étape de filtrage est nécessaire afin de réduire son impact. La solution classique, visant à remplacer la valeur d'un pixel de l'image par une moyenne pondérée des valeurs de ses voisins, présente un inconvénient majeur : comment choisir les pondérations adaptées alors que la structure du bruit varie d'un endroit à l'autre de l'image (à cause de la non-uniformité de l'illumination globale) ?<br/>
				Des chercheurs du studio Disney [<a href="#ref_1">1</a>,<a href="#ref_1">2</a>] ont tenté de résoudre ce problème en utilisant des algorithmes d'intelligence artificielle. L'objectif était de mettre au point un réseau de neurones convolutionnel capable de prédire le filtre (aussi appelé "noyau") adapté à chacun des pixels de l'image. Ainsi, l'étape de filtrage peut ensuite devenir "dynamique" dans le sens où la structure locale du bruit est prise en compte.<br/><br/>		
				</p>
				
				<p>
				L'architecture de la solution consiste en réalité en deux réseaux de neurones convolutionnels : un réseau dédié à la composante diffuse de l'image (part isotrope de la réflexion de la lumière) et un autre réseau dédié à la composante spéculaire (part anisotrope de la réflexion de la lumière). Les étapes de la méthode sont les suivantes :
				</p>
				<ol>
					<li>Séparation des données générées par le moteur de rendu : d'un côté les informations diffuses, de l'autre les informations spéculaires.</li>
					<li>Pré-traitement des données adapté à la catégorie (diffus ou spéculaire) : normalisation, passage au logarithme, etc.</li>
					<li>Passage de chacun des pixels (et de son voisinnage) dans les réseaux de neurones et application du filtrage.</li>
					<li>Post-traitement des deux images obtenues, visant à annuler le pré-traitement réalisé.</li>
					<li>Fusion des deux composantes (diffuse et spéculaire) et obtention de l'image finale débruitée.</li>
				</ol>
				
				
				<figure class="image" style="width:90%;">
						<img src="images/Denoising_model_full.png" width="100%"></img>
						<p>Architecture globale de la méthode de débruitage par génération de noyaux (<a href="https://studios.disneyresearch.com/2017/07/20/kernel-predicting-convolutional-networks-for-denoising-monte-carlo-renderings/">source</a>)</p>
				</figure>
				
				<p>
				Chacun des deux réseaux de neurones convolutionels est composée de 8 couches convolutionnelles cachées avec 100 filtres 5x5 à chaque couche. La fonction d'activation ReLu est utilisée en sortie de chaque couche. Le réseau prend en entrée les informations sur un pixel et son voisinage, et il fournit en sortie un noyau de taille 21x21, les pondérations étant finalement obtenues par normalisation (Softmax). Il suffit ensuite d'appliquer ce filtre au pixel. La fonction de perte choisie est la norme 1 entre le pixel généré et le pixel cible. L'optimisation se fait de manière classique.
				</p>
				
				<p>
				La jeu d'entrainement est constitué de 600 images calculées par <em>Path Tracing</em> avec plusieurs milliers de rayons par pixel (donc des images très peu bruitées) ayant demandé de nombreuses heures de calcul. Chacune des images est ensuite découpée en patchs. Les résultats sont très bons puisqu'à partir d'une image synthétisée avec 32 rayons par pixel, puis débruitée en 12 secondes, il est possible d'obtenir un résultat visuellement équivalent voire meilleur que la même image syntétisée avec 128 rayons par pixel ! Les gains de temps sont donc considérables, ce qui explique l'emploi massif de telles méthodes de débruitage dans l'industrie du cinéma d'animation aujourd'hui.
				</p>
				
				<figure class="image" style="width:100%;">
						<img src="images/Denoising_test_train.png" width="100%"></img>
						<p>(A gauche) Le jeu d'entraînement, composé d'images générées à 32 rayons par pixel, et de leur équivalent à 1024 rayons par pixel. (A droite) L'application de l'algorithme de débruitage sur une image à 32 rayons par pixel : le résultat est visuellement équivalent à une image à 1024 rayons par pixel (<a href="https://studios.disneyresearch.com/2017/07/20/kernel-predicting-convolutional-networks-for-denoising-monte-carlo-renderings/">source</a>)</p>
				</figure>
				
				  
			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container">
			  <h1 class="title">Tracé intelligent de contours</h1>
				  <h2 class="subtitle">
					Le machine learning pour aller aux Oscars</br>
				  </h2>
				  
				  <center>
				  <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Congratulations to the <a href="https://twitter.com/SpiderVerse?ref_src=twsrc%5Etfw">@SpiderVerse</a> cast and crew on their <a href="https://twitter.com/hashtag/Oscars?src=hash&amp;ref_src=twsrc%5Etfw">#Oscars</a> nomination for Best Animated Feature! 🕷️🎉 <a href="https://t.co/pstwTwNdHT">pic.twitter.com/pstwTwNdHT</a></p>&mdash; Sony Pictures Animation (@SonyAnimation) <a href="https://twitter.com/SonyAnimation/status/1087712954819108865?ref_src=twsrc%5Etfw">January 22, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
				  </center>
				  
				  
				<p>
				Lors de la production du film d'animation <em>Spiderman: Into the Spiderverse </em>, le studio Sony Pictures Imageworks a souhaité appliquer un style particulier à ses rendus, proche des illustrations des "comics". Cela nécessitait, entre autres, de superposer au rendu des "lignes d'encre" au niveau de certains contours (joues, nez, yeux, etc.). Cet effet est visible sur l'illustration du tweet ci-dessus. La difficulté réside dans le fait que ces lignes dépendent de l'angle de vue et de l'expression faciale du personnage.<br/>
				Dans un premier temps, un outil manuel a été développé, permettant aux artistes de dessiner ces "lignes d'encre" sur le rendu. Cependant, ce processus devant être fait image par image, cela demandait beaucoup de temps. C'est pourquoi des chercheurs du studio <a href="#ref_3">[3]</a> ont développé un outil d'intelligence artificielle, permettant de calculer automatiquement ces "lignes d'encre". Les artistes n'avaient alors plus qu'à réaliser les ajustements nécessaires.<br/><br/>
				</p>
				
				<p>
				Les maillages 3D, utilisés dans le secteur du cinéma d'animation, présentent l'avantage de pouvoir être "mis à plat" (passage du monde 3D à un espace 2D déformant le moins possible le maillage) par le biais d'une <em>paramétrisation</em>.<br/>
				Les détails sur la structure de l'algorithme utilisé n'ont pas été dévoilés par le studio. Ce dernier prend en entrée des informations sur l'orientation relative de la tête du personnage par rapport à la caméra ainsi que des informations sur l'expression faciale du personnage. Il fournit en sortie un ensemble de "lignes d'encre" dans l'espace de paramétrisation 2D, qu'il n'y a alors plus qu'à projeter dans le monde 3D par l'opération inverse.<br/><br/>
				</p>
				
				<p>
				Le jeu d'entrainement a été réalisé en plusieurs étapes :
				</p>
				<ol>
					<li>Tout d'abord, des artistes ont déssiné les "lignes d'encre" correspondant à 600 poses (position et expression faciale) différentes de différents personnages.</li>
					<li>Un premier algorithme a été entraîné sur ce jeu de données.</li>
					<li>Cet algorithme a été utilisé pour générer automatiquement une nouvelle série de "lignes d'encre" qui ont été corrigées par des artistes si nécessaire.</li>
					<li>Cet ensemble de "lignes d'encre" agrandi a permis d'entraîner un second algorithme plus performant que le premier. C'est ce dernier qui a finalement été utilisé lors de la production.</li>
				</ol>
				
				
				<figure class="image" style="width:70%;">
						<img src="images/miles_inklines_crop.jpg" width="100%"></img>
						<p>Un exemple de résultat final obtenu (<a href="https://www.fxguide.com/fxfeatured/ink-lines-and-machine-learning/?utm_source=twitter&utm_medium=social&utm_campaign=SocialWarfare">source</a>)</p>
				</figure>
				
				<p>
				L'intelligence artificielle a encore une fois été mise au service de l'artistique, permettant d'automatiser des tâches répétitives de haut niveau. Aucune solution classique n'avait permis d'obtenir des résultats équivalents.
				</p>
				  
			</div>
		</section>
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container">
			  <h1 class="title">Le transfert de style</h1>
				  <h2 class="subtitle">
					Une multitude de possibilités</br>
				  </h2>
				  
				<p>
				Le <em>Path Tracing</em> nécessite de décrire physiquement le comportement de la matière et permet d'obtenir des rendus très réalistes. Il est cependant très compliqué d'adapter cette technique dans le but d'obtenir un style artistique particulier. C'est dans ce cas que les récentes découvertes en terme de transfert de style peuvent être intéressantes.<br/><br/>
				</p>
				
				<p>
				En 2015, des chercheurs de l'université de Tübingen <a href="#ref_4">[4]</a> ont exploité les avancées dans le domaine de la classification d'images par réseaux de neurones profonds afin de créer un algorithme de transfert de style très efficace. Ces réseaux de neurones profonds, dédiés à la classification d'images, sont constitués d'un empilement de couches convolutionelles. A son arrivée dans chacune des couches, la sortie (l'équivalent d'une image) de la couche précédente subit plusieurs opérations de filtrage, menant à un ensemble de <em>feature maps</em> (l'équivalent d'une image) en sortie. Ces <em>feature maps</em> ont la particularité de comporter des informations précises sur le contenu et le style d'une image. Le contenu est décrit par la valeur des <em>feature maps</em>, les <em>feature maps</em> des couches profondes permettent d'identifier des concepts propres au contenu, particulièrement discriminants. Le style est décrit par la corrélation entre les <em>feature maps</em> d'une même couche.<br/>
				Ainsi, pour transférer le style d'une image sur une autre image, tout en conservant son contenu, il s'agit de résoudre un problème d'optimisation sur une fonction de perte définie à partie de ces <em>feature maps</em>. Un exemple de résultat est disponible sur l'image suivante.
				</p>
				
				<figure class="image" style="width:80%;">
						<img src="images/DeepArt_example.png" width="100%"></img>
						<p>Un exemple d'application de la méthode de transfert de style (<a href="https://arxiv.org/pdf/1508.06576.pdf">source</a>). Plusieurs styles artistiques (B,C,D) sont appliqués à l'image d'origine (A).</p>
				</figure>
				
				<p>
				Les perspectives offertes par cette méthode dans le secteur du cinéma d'animation paraissent intéressantes. Cependant, d'autres techniques, qui ne sont pas basées sur des algorithmes d'intelligence artificielle, permettent d'obtenir de meilleurs résultats. C'est le cas de StyleLit <a href="#ref_5">[5]</a>, mis au point par des chercheurs de l'université de Prague. Leur algorithme exploite les informations de "chemin d'un rayon lumineux" fournies par le moteur de rendu. Par exemple, un rayon lumineux arrivant sur l'oeil peut être parti d'une source de lumière (L), avoir subi 2 rebonds diffus (D) puis un rebond spéculaire (S) avant d'atteindre l'oeil (E). Son chemin sera alors décrit par l'expression <em>LDDSE</em>.<br/>
				Il suffit alors à un artiste de dessiner une scène simple avec le style voulu, puis de générer les expressions des chemins pour cette scène simple, pour avoir une table de correspondance entre expression et style. Il suffit ensuite d'appliquer ces styles aux endroits appropriés pour n'importe quelle nouvelle scène.
				</p>
				
				<figure class="image" style="width:90%;">
						<img src="images/StyleLit_example.png" width="100%"></img>
						<p>Résultats obtenus sans usage d'intelligence artificielle (<a href="https://dcgi.fel.cvut.cz/home/sykorad/stylit">source</a>). (Première image) La scène de référence. (Images suivantes) L'artiste dessine la scène simple (sphère dans l'encart en haut à droite de chaque image) et le style est appliqué à la scène. </p>
				</figure>
				
				<p>
				Les algorithmes de transfert de style utilisant les réseaux de neurones profonds dédiés à la classification n'ont pas accés à l'information de "chemin d'un rayon lumineux". Il pourrait toutefois être envisageable d'améliorer leurs résultats en exploitant ces informations, spécifiquement pour le cinéma d'animation.
				</p>
				  
			</div>
		</section>	
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		<section class="section">
			<div class="container refs">
				<h1 class="title">Références</h1>
				<ul>
					<li>[1] : DisneyResearch -
						<a id="ref_1" href="https://studios.disneyresearch.com/2017/07/20/kernel-predicting-convolutional-networks-for-denoising-monte-carlo-renderings/">Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings</a>
					</li>
					<li>[2] : DisneyResearch -
						<a id="ref_2"  href="https://studios.disneyresearch.com/2018/07/30/denoising-with-kernel-prediction-and-asymmetric-loss-functions/">Denoising with Kernel Prediction and Asymmetric Loss Functions</a>
					</li>
					<li>[3] : FXGuide -
						<a id="ref_3"  href="https://www.fxguide.com/fxfeatured/ink-lines-and-machine-learning/?utm_source=twitter&utm_medium=social&utm_campaign=SocialWarfare">Ink Lines and Machine Learning</a>
					</li>
					<li>[4] : Leon A. Gatys et al. -
						<a id="ref_4"  href="https://arxiv.org/pdf/1508.06576.pdf">A Neural Algorithm of Artistic Style</a>
					</li>
					<li>[5] : Jakub Fišer et al. -
						<a id="ref_5"  href="https://dcgi.fel.cvut.cz/home/sykorad/stylit">StyLit: Illumination-Guided Example-Based Stylization of 3D Renderings</a>
					</li>
				</ul>
			</div>
		</section>
		
	  </div>
	  <div class="column is-2"></div>
	</div>
	
	<footer class="columns">
		<div class="column is-2"></div>
		<div class="column is-8" id="footer_center">
			<div>© Grégoire GOBERT</div>
			<div>
				<a href="https://twitter.com/gobert_gregoire" style="color:rgb(29,161,242);font-size:1.3em;"><i class="fab fa-twitter"></i></a>
				<a href="https://github.com/GrgeoireGobert" style="color:#333;font-size:1.3em;"><i class="fab fa-github"></i></a>
				<a href="https://www.linkedin.com/in/greg-gobert/" style="color:rgb(40,103,178);font-size:1.3em;"><i class="fab fa-linkedin-in"></i></a>
			</div>
		</div>
		<div class="column is-2"></div>
	</footer>
	
  <script type="text/javascript" src="script.js"></script>
  </body>
</html>